experiment_name: "BioLinkBERT-base" # Descriptive name for this experiment run.
model_type: "huggingface" # or "bertlstmcrf" # Type of model to train.
model_name: "michiyasunaga/BioLinkBERT-base" # The Huggingface model identifier.
dataset_qualities: # Specify in this list which qualities of data should be included in the training dataset.
- "platinum"
- "gold"
- "silver"
- "bronze"
remove_html: True # Boolean deciding if HTML should be removed from the data
weighted_training: False # Boolean for if the training data is weighted or not
dataset_weights: # Weights for weighted training. Has to correspond to the datasets the number and order of datasets in preprocessing/main.py
- 1.2
- 1.2
- 1.0
- 0.33
find_optimal_lr: True # If True, a learning rate finder is run instead of training.
hyperparameters:
  num_epochs: 25 # Total number of full passes over the dataset.
  batch_size: 16 # Number of samples processed in one forward/backward pass.
  freeze_epochs: 0 # Number of epochs to freeze part of the model (e.g. embeddings + bottom layers).
  lr_scheduler:
    learning_rate: 0.0001 # Initial learning rate at the start of training.
    method: "custom" # Name of the scheduling strategy for adjusting the learning rate.
    # Supported values:
    # - "cosine annealing"     → smooth decay using cosine curve.
    # - "reduce on plateau"    → reduce LR if validation metric stops improving.
    # - "one cycle"            → dynamic warmup, peak, and decay in a single cycle.
    # - "custom"               → fully custom LR schedule.

    # ↓ Fields required by specific methods ↓

    # For 'cosine annealing':
    # num_epochs: Total number of training epochs for the scheduler to plan its decay.
    # min_learning_rate: The minimum learning rate the scheduer is allowed to reach.

    # For 'reduce on plateau':
    # factor: Factor by which the LR will be reduced.
    # patience: Number of epochs to wait before reducing LR.
    # threshold: Threshold for measuring new optimum, to ignore small improvements.

    # For 'one cycle':
    # max_learning_rate: The peak learning rate during the cycle.
    # pct_start: Fraction of the cycle spent increasing LR (default: 0.3).
    # anneal_strategy: Strategy to decay LR ("cos" for cosine, "linear" for linear).
    # div_factor: Initial LR is max_lr / div_factor (default: 25).
    # final_div_factor: Final LR is max_lr / final_div_factor (default: 1e4).

    # For 'custom':
    # LambdaLR for warmup and decay.
    # custom_schedule: Used for warmup with a list of 3-tuples (start_epoch, end_epoch, multiplier).
    # step_size: How many epochs before update the LR with a factor of gamma. Will update first time immediately after warmup. 
    # gamma: The factor to update the the LR with.
