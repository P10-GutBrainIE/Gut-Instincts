# ===========================
# Experiment Configuration Template
# ===========================

# Descriptive name for this experiment run.
experiment_name: "BioLinkBERT-base"

# The Huggingface model identifier or path to a pre-trained model.
model_name: "michiyasunaga/BioLinkBERT-base"

# Path to the preprocessed training dataset (pickle file expected).
training_data_path: "data_preprocessed/BioLinkBERT-base/training.pkl"

# Path to the preprocessed validation dataset (pickle file expected).
validation_data_path: "data_preprocessed/BioLinkBERT-base/validation.pkl"

# Boolean for if the training data is weighted or not
weighted_training: False

# Weights for weighted training. Has to correspond to the datasets the number and order of datasets in preprocessing/main.py
dataset_weights:
- 1.2
- 1.2
- 1.0
- 0.33

# ===========================
# Training Hyperparameters
# ===========================
hyperparameters:
  # Total number of full passes over the dataset.
  num_epochs: 25

  # Number of samples processed in one forward/backward pass.
  batch_size: 16

  # Number of epochs to freeze part of the model (e.g. embeddings + bottom layers).
  # This is useful for transfer learning — you can unfreeze later for fine-tuning.
  freeze_epochs: 0

  # ===========================
  # Learning Rate Scheduler Settings
  # ===========================
  lr_scheduler:

    # Name of the scheduling strategy for adjusting the learning rate.
    # Supported values:
    # - "cosine annealing"     → smooth decay using cosine curve.
    # - "reduce on plateau"    → reduce LR if validation metric stops improving.
    # - "custom"               → fully custom LR schedule.
    method: "cosine annealing"

    # Initial learning rate at the start of training.
    learning_rate: 0.0001

    # ↓ Fields required by specific methods ↓

    # For 'cosine annealing':
    # num_epochs: Total number of training epochs for the scheduler to plan its decay.
    # min_learning_rate: The minimum learning rate the scheduer is allowed to reach.

    # For 'reduce on plateau':
    # factor: Factor by which the LR will be reduced.
    # patience: Number of epochs to wait before reducing LR.
    # threshold: Threshold for measuring new optimum, to ignore small improvements.

    # For 'custom':
    # LambdaLR for warmup followed by StepLR.
    # custom_schedule: Used for warmup with a list of 3-tuples (start_epoch, end_epoch, multiplier).
    # step_size: When to update the LR with a factor of gamma.
    # gamma: The factor to update the the LR with.
